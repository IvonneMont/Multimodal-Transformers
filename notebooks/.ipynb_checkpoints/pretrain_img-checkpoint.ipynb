{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, BertModel\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "import random\n",
    "from transformers import BertConfig\n",
    "from models.mmbt import ImgBertClf\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args=args = argparse.Namespace(\n",
    "        dropout=0.1,\n",
    "        hidden_sz=768,\n",
    "        args.n_classes=13,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bertML = BertForMaskedLM.from_pretrained(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "outputs = model(**inputs, labels=labels,output_hidden_states=True)\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2=model.bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs2= m2(**inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.8050e-02,  1.7221e-02,  2.4272e-02, -2.0929e-01, -2.1915e-01,\n",
       "          1.2172e-01,  1.0628e-01,  5.8702e-01, -5.2350e-01, -7.1335e-02,\n",
       "         -7.0253e-02, -1.3331e-01,  8.3838e-02,  2.5778e-01,  4.6928e-02,\n",
       "          1.1478e-01, -3.0591e-01,  1.8163e-01,  1.6157e-01, -4.2178e-01,\n",
       "         -5.6703e-02,  8.0701e-02, -1.0021e-01, -2.4540e-03,  4.6200e-02,\n",
       "         -2.3278e-01, -5.2396e-02,  1.2881e-01,  2.3357e-01,  6.4706e-02,\n",
       "          4.1210e-02,  1.0281e-01, -9.0211e-02,  1.5799e-01,  2.4811e-01,\n",
       "          1.4406e-01, -1.1486e-01,  4.0731e-02,  2.3535e-01,  1.3785e-01,\n",
       "          8.4520e-02,  6.3605e-02,  1.4099e-01, -3.4659e-02,  2.1644e-01,\n",
       "         -2.1273e-01, -1.3931e+00,  1.3035e-01, -9.9831e-03,  2.6887e-02,\n",
       "         -2.0944e-01,  2.1997e-01,  3.3673e-01,  4.1214e-01,  1.9574e-01,\n",
       "          1.7287e-01, -4.3728e-01,  4.9616e-01,  2.1035e-02,  2.2385e-01,\n",
       "         -1.9457e-01,  2.2377e-01, -1.7232e-01,  1.8139e-01, -1.7394e-02,\n",
       "          2.8509e-01,  1.3049e-01,  2.0106e-02, -3.9996e-02,  1.8915e-01,\n",
       "         -2.8332e-01, -7.6137e-02,  3.7969e-02,  2.4476e-02, -4.4047e-02,\n",
       "         -1.7649e-01, -1.4630e-01,  1.9159e-01,  7.6301e-02,  1.1317e-02,\n",
       "          1.6890e-01,  5.4977e-02,  3.9131e-01,  6.2809e-02,  2.1050e-01,\n",
       "          2.6795e-01,  1.8034e-02,  8.1183e-02,  2.7930e-01,  3.4620e-01,\n",
       "         -3.3874e-01,  4.0134e-02,  7.3146e-02,  2.1016e-01,  2.7074e-02,\n",
       "         -2.7543e-01,  5.1851e-02, -3.2842e-02,  1.5848e-01,  1.7725e-01,\n",
       "         -2.2717e-02,  1.5302e-01,  7.6755e-02,  1.3409e-01, -1.9793e-01,\n",
       "          1.8901e-02, -2.9939e-01, -2.4161e-01, -1.0038e-01, -2.8408e+00,\n",
       "         -1.0732e-01,  2.4513e-02, -3.1067e-01, -9.4026e-02,  2.7032e-02,\n",
       "          3.2339e-01,  4.4476e-01,  6.6307e-02, -1.5539e-01, -1.4740e-01,\n",
       "         -9.0576e-02,  3.0294e-01, -8.7396e-02, -5.5351e-02,  4.3151e-02,\n",
       "          7.3785e-02,  9.7352e-02,  5.5009e-02,  7.6940e-02,  2.4770e-02,\n",
       "         -1.0211e-01,  2.1913e-01, -1.9850e-01, -3.1578e-02,  1.0141e-01,\n",
       "         -3.3301e-02,  2.5073e-01, -4.5382e-02,  3.1838e-01, -1.0259e-01,\n",
       "         -1.9704e-01,  6.4199e-02, -3.2925e+00,  2.2770e-01,  1.8089e-01,\n",
       "         -1.1523e-01, -6.2268e-02, -3.8516e-01,  1.1223e-01, -3.4458e-02,\n",
       "         -1.6859e-02, -1.4799e-01,  9.8620e-02,  4.1073e-01, -3.5204e-01,\n",
       "         -2.4727e-01,  3.5919e-01,  8.6209e-03,  4.0435e-01,  6.9793e-02,\n",
       "          1.5060e-01, -2.6703e-01,  3.8917e-01, -3.9823e-01,  5.4878e-04,\n",
       "          2.6223e-01,  4.0951e-01,  1.9908e-01,  1.4721e-01,  3.5625e-02,\n",
       "         -5.4383e-01, -5.5065e-02,  1.8824e-01, -2.0725e-01,  9.8772e-02,\n",
       "         -1.2689e-02, -1.5726e-02,  1.3569e-01,  1.5577e-02,  8.1780e-02,\n",
       "         -1.3637e-01,  4.1821e-01,  9.4602e-02,  3.1010e-01,  5.3189e-02,\n",
       "         -7.8502e-02,  1.6943e-01, -7.7724e-02,  4.5827e-02,  2.9964e-02,\n",
       "          1.3512e-01,  1.3105e-01,  2.1791e-02, -1.3862e-01,  6.2895e-02,\n",
       "          3.2570e-01, -1.4033e-01, -1.3204e-01,  2.6730e-01,  2.5853e-01,\n",
       "         -3.6373e-01, -2.4596e-01,  1.1009e-01,  3.3629e-02,  7.8241e-02,\n",
       "          3.5665e+00,  8.3121e-03, -2.7031e-01,  2.6027e-01,  2.3873e-01,\n",
       "          1.7532e-01,  2.0484e-02, -5.2252e-02,  6.1091e-04,  1.7283e-01,\n",
       "         -2.3599e-01, -1.2810e-01,  2.8922e-02,  9.6504e-02,  8.7128e-02,\n",
       "          2.3158e-02,  5.9304e-02,  3.7953e-03,  9.8688e-02,  3.8740e-02,\n",
       "          1.1814e-01,  2.5322e-01,  1.3070e-01, -4.0902e-01, -7.0464e-01,\n",
       "         -2.9071e-01,  3.6027e-02, -3.6535e-01,  2.4550e-01, -2.9739e-01,\n",
       "          1.7413e-02, -3.4953e-01, -1.6208e-01,  1.2328e-01, -9.5721e-02,\n",
       "          3.9220e-01,  2.2397e-02,  5.5771e-02,  1.1057e-01,  3.4168e-02,\n",
       "          2.8555e-01, -1.1764e-01,  7.4928e-02,  1.9301e-01,  8.5790e-02,\n",
       "          2.2189e-01, -2.7511e-01,  5.2831e-03, -1.5579e-02,  6.8571e-02,\n",
       "         -1.0265e-01,  2.3775e-01,  1.1074e-01,  2.9864e-02,  1.3311e-01,\n",
       "         -2.7433e-01, -3.0154e-02, -7.6318e-02,  1.4569e-01, -1.4790e-01,\n",
       "         -2.2186e-01,  1.5107e-01, -1.9987e-01,  1.3830e-01,  5.5308e-02,\n",
       "         -4.7189e-01,  6.2487e-02, -2.0613e-01, -5.1569e+00,  3.6215e-01,\n",
       "          2.3903e-01,  3.2041e-02,  1.8551e-01, -5.1761e-02,  4.0392e-01,\n",
       "          6.0793e-02,  2.6058e-01, -5.1768e-01,  4.0565e-02, -1.7581e-01,\n",
       "         -1.6480e-01,  2.2478e-01, -5.0171e-01, -4.6185e-02,  2.2417e-01,\n",
       "         -1.9310e-01, -4.4902e-01, -1.5327e-01,  2.4984e-01,  8.1542e-02,\n",
       "         -4.6562e-01,  1.5531e-01, -6.8442e-02, -1.2442e-01, -3.2292e-01,\n",
       "         -1.8474e-01,  6.2282e-02, -8.5401e-02,  2.1737e-01,  8.8975e-02,\n",
       "         -8.0864e-02,  1.1029e-01, -3.9244e-02, -1.7420e+00,  8.6037e-02,\n",
       "          2.3737e-01, -8.0233e-02,  1.3484e-02, -5.9952e-02,  2.0479e-01,\n",
       "         -3.6646e-02, -9.6848e-02, -1.0900e-01, -2.3914e-01, -2.7412e-02,\n",
       "         -3.0823e-01,  3.1965e-01,  2.4584e-01,  2.8906e-01,  8.4482e-02,\n",
       "         -7.4756e-02, -7.8181e-03,  8.9677e-02,  1.0671e-01,  1.2255e-01,\n",
       "         -5.3118e-02, -1.9183e-02,  1.7089e-01,  1.4885e-01, -2.8139e-01,\n",
       "         -2.0385e-01, -1.9845e-01, -1.1466e-01, -2.7180e-01, -1.8679e-02,\n",
       "         -4.2800e-02, -1.8820e-01,  2.8228e-03, -1.7499e-01,  1.5664e-01,\n",
       "          2.9825e-01,  6.3555e-02, -1.2851e-01, -2.0923e-01,  4.4984e-01,\n",
       "          2.6195e-01,  3.9858e-01,  2.1536e-01,  2.3733e-01, -9.4735e-02,\n",
       "         -2.0005e-01,  3.3890e-02,  1.9207e-01, -1.1444e-01, -2.9048e-02,\n",
       "          8.5587e-01, -4.8529e-02,  1.9299e-01, -4.3852e-01,  3.3408e-01,\n",
       "          9.1218e-02,  1.6110e-01,  4.1056e-01,  3.0644e-01,  3.9268e-01,\n",
       "          5.3083e-02, -1.3702e-01,  1.8605e-02, -1.2577e-01, -1.5277e-01,\n",
       "          1.7390e-01,  1.5055e-02, -1.1259e-02, -7.6551e-02,  1.9480e-02,\n",
       "         -1.4456e-01, -2.9237e-01, -2.6688e-01, -1.8612e-01, -1.5886e-01,\n",
       "          7.0810e-02,  2.2442e-01,  2.2633e-01, -2.7820e-01,  5.6848e-02,\n",
       "          1.8658e-01,  1.1954e-01, -1.1291e-01,  1.0650e-01, -1.2537e-01,\n",
       "          1.0172e-02, -9.7908e-02, -4.5779e-02, -1.8908e-01,  1.4713e-01,\n",
       "          2.4341e-01,  4.2266e-02, -1.6410e-01, -2.1944e-01,  4.6481e-02,\n",
       "         -6.4513e-01, -1.4312e-01, -2.4465e-02, -1.4176e-01, -1.6227e-01,\n",
       "         -1.3891e-01,  1.6070e-01, -1.6343e-01, -7.9681e-02,  1.4271e-01,\n",
       "          4.9813e-01, -1.2969e-01,  3.0525e-01, -1.0797e-01,  9.6808e-02,\n",
       "          6.2578e-03,  4.1615e-02,  7.7762e-01, -3.1597e-01,  6.0173e-02,\n",
       "          2.4262e-01, -3.4228e-01, -5.5425e-03,  3.8673e-03,  1.7338e-01,\n",
       "          1.4888e-01, -5.1113e-02, -4.8998e-02,  1.4126e-01, -1.9389e-01,\n",
       "         -3.5511e-01, -1.6306e-01, -6.2621e-02,  8.0612e-02, -8.3181e-02,\n",
       "          9.1705e-02, -3.7418e-01,  1.9588e-01, -1.8548e-01, -2.6960e-03,\n",
       "          1.6774e-01,  2.0192e-01,  1.2862e-01,  3.9410e-01,  1.3192e-01,\n",
       "          1.6637e-01,  4.0581e-01,  2.9725e-01,  1.2264e-01,  1.7377e-01,\n",
       "          3.9760e-02,  5.3575e-02, -9.4897e-03,  9.3432e-02, -6.2503e-03,\n",
       "          2.0733e-02,  3.1827e-02, -1.5422e-01,  1.3666e-01,  2.0364e-01,\n",
       "          8.2192e-02,  1.2805e-01,  3.2703e-02,  3.4871e-02, -1.5001e-01,\n",
       "         -6.9835e-01,  4.5893e-01,  2.7705e-01,  9.1987e-02, -3.4058e-03,\n",
       "         -1.8147e-01, -9.9142e-02,  1.7173e-01,  1.5885e-01, -4.2400e-02,\n",
       "         -1.1520e-01, -5.9583e-02, -9.0237e-02, -3.0237e-01,  3.5818e-01,\n",
       "          4.3873e-02, -1.1775e-02, -2.1857e-01,  2.0489e-01,  1.2267e-02,\n",
       "          7.2838e-02,  3.7705e-01, -2.0219e-01, -2.1997e-01,  1.5885e-01,\n",
       "          3.2514e-02,  4.8477e-01,  8.0807e-02,  1.3438e-01,  2.7087e-01,\n",
       "         -2.4296e-01, -1.8945e-01, -5.6610e-01, -2.6902e-01, -1.5726e-02,\n",
       "         -2.1155e-01,  7.2236e-02,  9.7704e-02,  1.1313e-01,  6.4620e-02,\n",
       "          3.1792e-02,  2.0204e-01, -1.7586e-03, -1.2142e-01,  2.5622e-01,\n",
       "         -1.0119e-01, -6.9760e-02,  7.1779e-02, -2.1841e-01, -1.0995e-01,\n",
       "         -3.7688e-02,  3.0185e-01, -2.4374e-01, -8.6071e-02,  4.9672e-02,\n",
       "          4.3384e-02, -1.4055e-01, -1.2016e-03, -2.4370e-01, -1.6730e-01,\n",
       "          7.4938e-01, -1.5932e-01,  3.6618e-02,  5.4102e-01, -2.5719e-01,\n",
       "         -4.0501e-01, -1.2180e-01,  4.9689e-02,  9.6828e-02,  5.7554e-02,\n",
       "          8.6987e-02, -1.7242e-01, -9.6642e-02, -9.3953e-02, -6.0178e-02,\n",
       "          7.4114e-02, -2.0672e-01, -1.7449e-01,  3.0998e-02, -2.0706e-02,\n",
       "          1.5548e-01, -2.7484e-01, -2.7600e-01, -1.0415e-01,  3.0511e-01,\n",
       "          4.6275e-02, -2.5088e-01,  2.7203e-02, -4.3764e-01, -7.8576e-03,\n",
       "          6.9321e-02, -1.2752e-01,  2.4089e-01,  1.4676e-01,  6.1716e-02,\n",
       "          3.1617e-01,  2.4385e-01,  1.4738e-01,  9.7848e-02, -2.7943e-01,\n",
       "         -3.2460e-02,  5.5350e-01,  1.2429e-01, -4.2344e-01, -3.3954e-02,\n",
       "          2.3077e-01,  3.0962e-01, -9.8935e-02, -8.8156e-02,  3.4721e-02,\n",
       "         -2.9715e-01,  3.0711e-01, -5.4755e-03,  4.0039e-02,  3.3160e-02,\n",
       "          2.4859e-01,  2.5948e-01,  5.8421e-02,  2.5898e+00,  4.9900e-01,\n",
       "          1.8856e-01, -1.7327e-01,  4.2640e-01, -6.6365e-02,  9.9928e-02,\n",
       "          1.8267e-01, -3.0165e-01,  1.4653e-02,  2.4188e-01, -2.4439e-01,\n",
       "         -1.4680e-03,  1.4461e-01,  1.2798e-01, -1.4537e-01, -2.3887e-01,\n",
       "          3.3796e-02, -8.3215e-02,  1.3900e-01, -5.6350e-01,  3.2625e-01,\n",
       "          1.7470e-01, -2.9273e-01,  9.1564e-02, -2.7492e-04, -1.8289e-02,\n",
       "         -6.1166e-02,  2.1596e-01, -1.0911e-01,  1.7132e-01, -1.5132e-01,\n",
       "         -1.6146e-01,  2.2207e-01,  1.1415e-01,  2.6530e-01, -2.9588e-02,\n",
       "         -3.7362e-01, -3.4305e-02,  1.2250e-01, -2.4413e-01, -3.9383e-01,\n",
       "         -5.9051e-02, -1.2048e-01,  1.7101e-01,  2.6361e-01,  2.9251e-01,\n",
       "          6.5514e-02,  1.4372e-01, -1.4366e-01, -9.4920e-02,  7.1389e-02,\n",
       "         -1.0875e-01,  9.3752e-02, -4.6283e-01, -1.7109e-01,  1.9509e-02,\n",
       "          2.1922e-01, -4.4942e-02,  3.0462e-01,  1.8113e-01,  1.3215e-01,\n",
       "         -1.1029e-01, -1.1083e-01,  2.9618e-01,  1.9114e-02, -1.8782e-01,\n",
       "         -5.2251e-02,  8.5531e-02, -2.6728e-01,  3.3652e-01, -5.5108e-02,\n",
       "          5.2947e-02,  2.7539e-01,  2.6461e-01, -4.2309e-01,  2.1541e-01,\n",
       "          9.8092e-02, -5.5126e-02, -3.5143e+00, -7.1295e-02, -4.8115e-02,\n",
       "          9.8810e-02,  3.1679e-01,  2.6178e-01,  8.6487e-02,  1.4398e-01,\n",
       "         -1.1286e-02, -1.8073e-01,  1.9905e-01,  4.7717e-02,  1.4390e-01,\n",
       "          4.3127e-01, -4.5589e-02,  2.2850e-01, -9.8497e-02, -4.6151e-01,\n",
       "         -7.0624e-03, -5.4923e-02, -3.0657e-01, -5.2898e-02, -3.1365e-01,\n",
       "         -1.0045e-02, -1.6354e-01, -1.6280e-01,  7.1651e-02, -3.1428e-02,\n",
       "         -9.4093e-02, -5.4449e-02, -1.1948e-01,  1.2094e-01, -6.2941e-03,\n",
       "          2.6431e-01, -1.1597e-01, -4.5273e-02, -4.1110e-02,  1.2006e-01,\n",
       "          2.3457e-01,  9.6235e-02, -1.2326e-01,  3.6381e-01, -8.3225e-02,\n",
       "         -2.9759e-01,  3.1910e-02,  2.0673e-01,  5.7346e-02, -1.9479e-01,\n",
       "         -3.8691e-02, -4.0495e-01, -2.2776e-01,  5.1731e-02, -4.1874e-02,\n",
       "          1.1064e-01,  4.2469e-01,  4.0256e-01, -1.8612e-01,  1.1774e-01,\n",
       "         -5.5855e-02, -6.4244e-02,  2.3732e-01,  1.5427e-01,  5.8352e-02,\n",
       "          1.0548e-01,  1.5902e-01, -2.7728e-02, -7.5340e-02, -3.1461e-03,\n",
       "          5.9734e-02, -6.9644e-02, -8.9080e-03, -4.1965e-02, -1.7530e-01,\n",
       "          7.3289e-03,  1.6403e-01, -1.4567e-01, -2.4860e-02,  2.0964e-01,\n",
       "          3.5268e-01, -1.6565e-01, -3.8113e-02, -4.8202e-01,  8.2635e-02,\n",
       "          6.7411e-02,  2.5000e-02, -9.6654e+00, -1.9350e-01, -2.7340e-02,\n",
       "         -1.2743e-01,  1.0318e-01, -6.7492e-02, -8.7299e-02, -7.3068e-03,\n",
       "         -9.5101e-02,  3.2404e-02,  3.2165e-01, -2.3627e-01,  3.1101e-01,\n",
       "         -8.7657e-02,  1.9433e-01,  2.8283e-01]], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs2.last_hidden_state[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-725fbccde4a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "sequence_output = encoder_outputs[0]\n",
    "pooled_output = self.pooler(sequence_output) if self.pooler is not None else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-db0b1b6e9ae1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0moutputs2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mloss2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mlogits2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutputWithPoolingAndCrossAttentions' object has no attribute 'loss'"
     ]
    }
   ],
   "source": [
    "model2 = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "outputs2 = outputs = model(**inputs, labels=labels,output_hidden_states=True)\n",
    "loss2 = outputs2.loss\n",
    "logits2 = outputs2.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 768])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs2.last_hidden_state.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs2[1].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs=torch.stack(outputs.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 768])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs[-1, :,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.8050e-02,  1.7221e-02,  2.4272e-02, -2.0929e-01, -2.1915e-01,\n",
       "          1.2172e-01,  1.0628e-01,  5.8702e-01, -5.2350e-01, -7.1335e-02,\n",
       "         -7.0253e-02, -1.3331e-01,  8.3838e-02,  2.5778e-01,  4.6928e-02,\n",
       "          1.1478e-01, -3.0591e-01,  1.8163e-01,  1.6157e-01, -4.2178e-01,\n",
       "         -5.6703e-02,  8.0701e-02, -1.0021e-01, -2.4540e-03,  4.6200e-02,\n",
       "         -2.3278e-01, -5.2396e-02,  1.2881e-01,  2.3357e-01,  6.4706e-02,\n",
       "          4.1210e-02,  1.0281e-01, -9.0211e-02,  1.5799e-01,  2.4811e-01,\n",
       "          1.4406e-01, -1.1486e-01,  4.0731e-02,  2.3535e-01,  1.3785e-01,\n",
       "          8.4520e-02,  6.3605e-02,  1.4099e-01, -3.4659e-02,  2.1644e-01,\n",
       "         -2.1273e-01, -1.3931e+00,  1.3035e-01, -9.9831e-03,  2.6887e-02,\n",
       "         -2.0944e-01,  2.1997e-01,  3.3673e-01,  4.1214e-01,  1.9574e-01,\n",
       "          1.7287e-01, -4.3728e-01,  4.9616e-01,  2.1035e-02,  2.2385e-01,\n",
       "         -1.9457e-01,  2.2377e-01, -1.7232e-01,  1.8139e-01, -1.7394e-02,\n",
       "          2.8509e-01,  1.3049e-01,  2.0106e-02, -3.9996e-02,  1.8915e-01,\n",
       "         -2.8332e-01, -7.6137e-02,  3.7969e-02,  2.4476e-02, -4.4047e-02,\n",
       "         -1.7649e-01, -1.4630e-01,  1.9159e-01,  7.6301e-02,  1.1317e-02,\n",
       "          1.6890e-01,  5.4977e-02,  3.9131e-01,  6.2809e-02,  2.1050e-01,\n",
       "          2.6795e-01,  1.8034e-02,  8.1183e-02,  2.7930e-01,  3.4620e-01,\n",
       "         -3.3874e-01,  4.0134e-02,  7.3146e-02,  2.1016e-01,  2.7074e-02,\n",
       "         -2.7543e-01,  5.1851e-02, -3.2842e-02,  1.5848e-01,  1.7725e-01,\n",
       "         -2.2717e-02,  1.5302e-01,  7.6755e-02,  1.3409e-01, -1.9793e-01,\n",
       "          1.8901e-02, -2.9939e-01, -2.4161e-01, -1.0038e-01, -2.8408e+00,\n",
       "         -1.0732e-01,  2.4513e-02, -3.1067e-01, -9.4026e-02,  2.7032e-02,\n",
       "          3.2339e-01,  4.4476e-01,  6.6307e-02, -1.5539e-01, -1.4740e-01,\n",
       "         -9.0576e-02,  3.0294e-01, -8.7396e-02, -5.5351e-02,  4.3151e-02,\n",
       "          7.3785e-02,  9.7352e-02,  5.5009e-02,  7.6940e-02,  2.4770e-02,\n",
       "         -1.0211e-01,  2.1913e-01, -1.9850e-01, -3.1578e-02,  1.0141e-01,\n",
       "         -3.3301e-02,  2.5073e-01, -4.5382e-02,  3.1838e-01, -1.0259e-01,\n",
       "         -1.9704e-01,  6.4199e-02, -3.2925e+00,  2.2770e-01,  1.8089e-01,\n",
       "         -1.1523e-01, -6.2268e-02, -3.8516e-01,  1.1223e-01, -3.4458e-02,\n",
       "         -1.6859e-02, -1.4799e-01,  9.8620e-02,  4.1073e-01, -3.5204e-01,\n",
       "         -2.4727e-01,  3.5919e-01,  8.6209e-03,  4.0435e-01,  6.9793e-02,\n",
       "          1.5060e-01, -2.6703e-01,  3.8917e-01, -3.9823e-01,  5.4878e-04,\n",
       "          2.6223e-01,  4.0951e-01,  1.9908e-01,  1.4721e-01,  3.5625e-02,\n",
       "         -5.4383e-01, -5.5065e-02,  1.8824e-01, -2.0725e-01,  9.8772e-02,\n",
       "         -1.2689e-02, -1.5726e-02,  1.3569e-01,  1.5577e-02,  8.1780e-02,\n",
       "         -1.3637e-01,  4.1821e-01,  9.4602e-02,  3.1010e-01,  5.3189e-02,\n",
       "         -7.8502e-02,  1.6943e-01, -7.7724e-02,  4.5827e-02,  2.9964e-02,\n",
       "          1.3512e-01,  1.3105e-01,  2.1791e-02, -1.3862e-01,  6.2895e-02,\n",
       "          3.2570e-01, -1.4033e-01, -1.3204e-01,  2.6730e-01,  2.5853e-01,\n",
       "         -3.6373e-01, -2.4596e-01,  1.1009e-01,  3.3629e-02,  7.8241e-02,\n",
       "          3.5665e+00,  8.3121e-03, -2.7031e-01,  2.6027e-01,  2.3873e-01,\n",
       "          1.7532e-01,  2.0484e-02, -5.2252e-02,  6.1091e-04,  1.7283e-01,\n",
       "         -2.3599e-01, -1.2810e-01,  2.8922e-02,  9.6504e-02,  8.7128e-02,\n",
       "          2.3158e-02,  5.9304e-02,  3.7953e-03,  9.8688e-02,  3.8740e-02,\n",
       "          1.1814e-01,  2.5322e-01,  1.3070e-01, -4.0902e-01, -7.0464e-01,\n",
       "         -2.9071e-01,  3.6027e-02, -3.6535e-01,  2.4550e-01, -2.9739e-01,\n",
       "          1.7413e-02, -3.4953e-01, -1.6208e-01,  1.2328e-01, -9.5721e-02,\n",
       "          3.9220e-01,  2.2397e-02,  5.5771e-02,  1.1057e-01,  3.4168e-02,\n",
       "          2.8555e-01, -1.1764e-01,  7.4928e-02,  1.9301e-01,  8.5790e-02,\n",
       "          2.2189e-01, -2.7511e-01,  5.2831e-03, -1.5579e-02,  6.8571e-02,\n",
       "         -1.0265e-01,  2.3775e-01,  1.1074e-01,  2.9864e-02,  1.3311e-01,\n",
       "         -2.7433e-01, -3.0154e-02, -7.6318e-02,  1.4569e-01, -1.4790e-01,\n",
       "         -2.2186e-01,  1.5107e-01, -1.9987e-01,  1.3830e-01,  5.5308e-02,\n",
       "         -4.7189e-01,  6.2487e-02, -2.0613e-01, -5.1569e+00,  3.6215e-01,\n",
       "          2.3903e-01,  3.2041e-02,  1.8551e-01, -5.1761e-02,  4.0392e-01,\n",
       "          6.0793e-02,  2.6058e-01, -5.1768e-01,  4.0565e-02, -1.7581e-01,\n",
       "         -1.6480e-01,  2.2478e-01, -5.0171e-01, -4.6185e-02,  2.2417e-01,\n",
       "         -1.9310e-01, -4.4902e-01, -1.5327e-01,  2.4984e-01,  8.1542e-02,\n",
       "         -4.6562e-01,  1.5531e-01, -6.8442e-02, -1.2442e-01, -3.2292e-01,\n",
       "         -1.8474e-01,  6.2282e-02, -8.5401e-02,  2.1737e-01,  8.8975e-02,\n",
       "         -8.0864e-02,  1.1029e-01, -3.9244e-02, -1.7420e+00,  8.6037e-02,\n",
       "          2.3737e-01, -8.0233e-02,  1.3484e-02, -5.9952e-02,  2.0479e-01,\n",
       "         -3.6646e-02, -9.6848e-02, -1.0900e-01, -2.3914e-01, -2.7412e-02,\n",
       "         -3.0823e-01,  3.1965e-01,  2.4584e-01,  2.8906e-01,  8.4482e-02,\n",
       "         -7.4756e-02, -7.8181e-03,  8.9677e-02,  1.0671e-01,  1.2255e-01,\n",
       "         -5.3118e-02, -1.9183e-02,  1.7089e-01,  1.4885e-01, -2.8139e-01,\n",
       "         -2.0385e-01, -1.9845e-01, -1.1466e-01, -2.7180e-01, -1.8679e-02,\n",
       "         -4.2800e-02, -1.8820e-01,  2.8228e-03, -1.7499e-01,  1.5664e-01,\n",
       "          2.9825e-01,  6.3555e-02, -1.2851e-01, -2.0923e-01,  4.4984e-01,\n",
       "          2.6195e-01,  3.9858e-01,  2.1536e-01,  2.3733e-01, -9.4735e-02,\n",
       "         -2.0005e-01,  3.3890e-02,  1.9207e-01, -1.1444e-01, -2.9048e-02,\n",
       "          8.5587e-01, -4.8529e-02,  1.9299e-01, -4.3852e-01,  3.3408e-01,\n",
       "          9.1218e-02,  1.6110e-01,  4.1056e-01,  3.0644e-01,  3.9268e-01,\n",
       "          5.3083e-02, -1.3702e-01,  1.8605e-02, -1.2577e-01, -1.5277e-01,\n",
       "          1.7390e-01,  1.5055e-02, -1.1259e-02, -7.6551e-02,  1.9480e-02,\n",
       "         -1.4456e-01, -2.9237e-01, -2.6688e-01, -1.8612e-01, -1.5886e-01,\n",
       "          7.0810e-02,  2.2442e-01,  2.2633e-01, -2.7820e-01,  5.6848e-02,\n",
       "          1.8658e-01,  1.1954e-01, -1.1291e-01,  1.0650e-01, -1.2537e-01,\n",
       "          1.0172e-02, -9.7908e-02, -4.5779e-02, -1.8908e-01,  1.4713e-01,\n",
       "          2.4341e-01,  4.2266e-02, -1.6410e-01, -2.1944e-01,  4.6481e-02,\n",
       "         -6.4513e-01, -1.4312e-01, -2.4465e-02, -1.4176e-01, -1.6227e-01,\n",
       "         -1.3891e-01,  1.6070e-01, -1.6343e-01, -7.9681e-02,  1.4271e-01,\n",
       "          4.9813e-01, -1.2969e-01,  3.0525e-01, -1.0797e-01,  9.6808e-02,\n",
       "          6.2578e-03,  4.1615e-02,  7.7762e-01, -3.1597e-01,  6.0173e-02,\n",
       "          2.4262e-01, -3.4228e-01, -5.5425e-03,  3.8673e-03,  1.7338e-01,\n",
       "          1.4888e-01, -5.1113e-02, -4.8998e-02,  1.4126e-01, -1.9389e-01,\n",
       "         -3.5511e-01, -1.6306e-01, -6.2621e-02,  8.0612e-02, -8.3181e-02,\n",
       "          9.1705e-02, -3.7418e-01,  1.9588e-01, -1.8548e-01, -2.6960e-03,\n",
       "          1.6774e-01,  2.0192e-01,  1.2862e-01,  3.9410e-01,  1.3192e-01,\n",
       "          1.6637e-01,  4.0581e-01,  2.9725e-01,  1.2264e-01,  1.7377e-01,\n",
       "          3.9760e-02,  5.3575e-02, -9.4897e-03,  9.3432e-02, -6.2503e-03,\n",
       "          2.0733e-02,  3.1827e-02, -1.5422e-01,  1.3666e-01,  2.0364e-01,\n",
       "          8.2192e-02,  1.2805e-01,  3.2703e-02,  3.4871e-02, -1.5001e-01,\n",
       "         -6.9835e-01,  4.5893e-01,  2.7705e-01,  9.1987e-02, -3.4058e-03,\n",
       "         -1.8147e-01, -9.9142e-02,  1.7173e-01,  1.5885e-01, -4.2400e-02,\n",
       "         -1.1520e-01, -5.9583e-02, -9.0237e-02, -3.0237e-01,  3.5818e-01,\n",
       "          4.3873e-02, -1.1775e-02, -2.1857e-01,  2.0489e-01,  1.2267e-02,\n",
       "          7.2838e-02,  3.7705e-01, -2.0219e-01, -2.1997e-01,  1.5885e-01,\n",
       "          3.2514e-02,  4.8477e-01,  8.0807e-02,  1.3438e-01,  2.7087e-01,\n",
       "         -2.4296e-01, -1.8945e-01, -5.6610e-01, -2.6902e-01, -1.5726e-02,\n",
       "         -2.1155e-01,  7.2236e-02,  9.7704e-02,  1.1313e-01,  6.4620e-02,\n",
       "          3.1792e-02,  2.0204e-01, -1.7586e-03, -1.2142e-01,  2.5622e-01,\n",
       "         -1.0119e-01, -6.9760e-02,  7.1779e-02, -2.1841e-01, -1.0995e-01,\n",
       "         -3.7688e-02,  3.0185e-01, -2.4374e-01, -8.6071e-02,  4.9672e-02,\n",
       "          4.3384e-02, -1.4055e-01, -1.2016e-03, -2.4370e-01, -1.6730e-01,\n",
       "          7.4938e-01, -1.5932e-01,  3.6618e-02,  5.4102e-01, -2.5719e-01,\n",
       "         -4.0501e-01, -1.2180e-01,  4.9689e-02,  9.6828e-02,  5.7554e-02,\n",
       "          8.6987e-02, -1.7242e-01, -9.6642e-02, -9.3953e-02, -6.0178e-02,\n",
       "          7.4114e-02, -2.0672e-01, -1.7449e-01,  3.0998e-02, -2.0706e-02,\n",
       "          1.5548e-01, -2.7484e-01, -2.7600e-01, -1.0415e-01,  3.0511e-01,\n",
       "          4.6275e-02, -2.5088e-01,  2.7203e-02, -4.3764e-01, -7.8576e-03,\n",
       "          6.9321e-02, -1.2752e-01,  2.4089e-01,  1.4676e-01,  6.1716e-02,\n",
       "          3.1617e-01,  2.4385e-01,  1.4738e-01,  9.7848e-02, -2.7943e-01,\n",
       "         -3.2460e-02,  5.5350e-01,  1.2429e-01, -4.2344e-01, -3.3954e-02,\n",
       "          2.3077e-01,  3.0962e-01, -9.8935e-02, -8.8156e-02,  3.4721e-02,\n",
       "         -2.9715e-01,  3.0711e-01, -5.4755e-03,  4.0039e-02,  3.3160e-02,\n",
       "          2.4859e-01,  2.5948e-01,  5.8421e-02,  2.5898e+00,  4.9900e-01,\n",
       "          1.8856e-01, -1.7327e-01,  4.2640e-01, -6.6365e-02,  9.9928e-02,\n",
       "          1.8267e-01, -3.0165e-01,  1.4653e-02,  2.4188e-01, -2.4439e-01,\n",
       "         -1.4680e-03,  1.4461e-01,  1.2798e-01, -1.4537e-01, -2.3887e-01,\n",
       "          3.3796e-02, -8.3215e-02,  1.3900e-01, -5.6350e-01,  3.2625e-01,\n",
       "          1.7470e-01, -2.9273e-01,  9.1564e-02, -2.7492e-04, -1.8289e-02,\n",
       "         -6.1166e-02,  2.1596e-01, -1.0911e-01,  1.7132e-01, -1.5132e-01,\n",
       "         -1.6146e-01,  2.2207e-01,  1.1415e-01,  2.6530e-01, -2.9588e-02,\n",
       "         -3.7362e-01, -3.4305e-02,  1.2250e-01, -2.4413e-01, -3.9383e-01,\n",
       "         -5.9051e-02, -1.2048e-01,  1.7101e-01,  2.6361e-01,  2.9251e-01,\n",
       "          6.5514e-02,  1.4372e-01, -1.4366e-01, -9.4920e-02,  7.1389e-02,\n",
       "         -1.0875e-01,  9.3752e-02, -4.6283e-01, -1.7109e-01,  1.9509e-02,\n",
       "          2.1922e-01, -4.4942e-02,  3.0462e-01,  1.8113e-01,  1.3215e-01,\n",
       "         -1.1029e-01, -1.1083e-01,  2.9618e-01,  1.9114e-02, -1.8782e-01,\n",
       "         -5.2251e-02,  8.5531e-02, -2.6728e-01,  3.3652e-01, -5.5108e-02,\n",
       "          5.2947e-02,  2.7539e-01,  2.6461e-01, -4.2309e-01,  2.1541e-01,\n",
       "          9.8092e-02, -5.5126e-02, -3.5143e+00, -7.1295e-02, -4.8115e-02,\n",
       "          9.8810e-02,  3.1679e-01,  2.6178e-01,  8.6487e-02,  1.4398e-01,\n",
       "         -1.1286e-02, -1.8073e-01,  1.9905e-01,  4.7717e-02,  1.4390e-01,\n",
       "          4.3127e-01, -4.5589e-02,  2.2850e-01, -9.8497e-02, -4.6151e-01,\n",
       "         -7.0624e-03, -5.4923e-02, -3.0657e-01, -5.2898e-02, -3.1365e-01,\n",
       "         -1.0045e-02, -1.6354e-01, -1.6280e-01,  7.1651e-02, -3.1428e-02,\n",
       "         -9.4093e-02, -5.4449e-02, -1.1948e-01,  1.2094e-01, -6.2941e-03,\n",
       "          2.6431e-01, -1.1597e-01, -4.5273e-02, -4.1110e-02,  1.2006e-01,\n",
       "          2.3457e-01,  9.6235e-02, -1.2326e-01,  3.6381e-01, -8.3225e-02,\n",
       "         -2.9759e-01,  3.1910e-02,  2.0673e-01,  5.7346e-02, -1.9479e-01,\n",
       "         -3.8691e-02, -4.0495e-01, -2.2776e-01,  5.1731e-02, -4.1874e-02,\n",
       "          1.1064e-01,  4.2469e-01,  4.0256e-01, -1.8612e-01,  1.1774e-01,\n",
       "         -5.5855e-02, -6.4244e-02,  2.3732e-01,  1.5427e-01,  5.8352e-02,\n",
       "          1.0548e-01,  1.5902e-01, -2.7728e-02, -7.5340e-02, -3.1461e-03,\n",
       "          5.9734e-02, -6.9644e-02, -8.9080e-03, -4.1965e-02, -1.7530e-01,\n",
       "          7.3289e-03,  1.6403e-01, -1.4567e-01, -2.4860e-02,  2.0964e-01,\n",
       "          3.5268e-01, -1.6565e-01, -3.8113e-02, -4.8202e-01,  8.2635e-02,\n",
       "          6.7411e-02,  2.5000e-02, -9.6654e+00, -1.9350e-01, -2.7340e-02,\n",
       "         -1.2743e-01,  1.0318e-01, -6.7492e-02, -8.7299e-02, -7.3068e-03,\n",
       "         -9.5101e-02,  3.2404e-02,  3.2165e-01, -2.3627e-01,  3.1101e-01,\n",
       "         -8.7657e-02,  1.9433e-01,  2.8283e-01]], grad_fn=<SliceBackward>)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs[-1][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hs=torch.stack(hs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13, 1, 9, 768])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hs.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=[json.loads(l) for l in open(\"data_sets/proc/moviescope/train.jsonl\")]\n",
    "val=[json.loads(l) for l in open(\"data_sets/proc/moviescope/dev.jsonl\")]\n",
    "test=[json.loads(l) for l in open(\"data_sets/proc/moviescope/test.jsonl\")]\n",
    "data_dir=\"data_sets/proc/moviescope/img_indices2/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        # store encodings internally\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __len__(self):\n",
    "        # return the number of samples\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # return dictionary of input_ids, attention_mask, and labels for index i\n",
    "        return {key: tensor[i] for key, tensor in self.encodings.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(input_ids):\n",
    "    output_tokens=input_ids.clone().detach()\n",
    "    choises_idx=torch.randperm(256)\n",
    "    num_mask=0\n",
    "    max_mask=int(0.15*(input_ids.size()[0]))\n",
    "    for index in choises_idx:\n",
    "        if num_mask==max_mask:\n",
    "            break\n",
    "        # 80% of the time, replace with [MASK]\n",
    "        if random.uniform(0, 1)< 0.8:\n",
    "            masked_token = 3\n",
    "        else:\n",
    "            # 10% of the time, keep original\n",
    "            if random.uniform(0, 1) < 0.5:\n",
    "                masked_token = input_ids[index]\n",
    "            # 10% of the time, replace with random word\n",
    "            else:\n",
    "                masked_token = random.randint(0,1023)\n",
    "\n",
    "        output_tokens[index] = masked_token\n",
    "        num_mask+=1\n",
    "    labels = input_ids.masked_fill( output_tokens != 3, -100)\n",
    "    return  output_tokens, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepo_data(data,data_dir):\n",
    "    len_data=len(data)\n",
    "    imgs_ids=[]\n",
    "    for d in data:\n",
    "        path=data_dir+str(d[\"id\"])+\".pt\"\n",
    "        img_ids= torch.load(path).long()\n",
    "        imgs_ids.append(img_ids)\n",
    "    tensor_ids=torch.stack(imgs_ids)+4\n",
    "    tensor_ids_mask=[]\n",
    "    labels_mask=[]\n",
    "    for t in tensor_ids:\n",
    "        im, lm= mask(t)\n",
    "        tensor_ids_mask.append(im)\n",
    "        labels_mask.append(lm)\n",
    "    tensor_ids_mask=torch.stack(tensor_ids_mask)\n",
    "    labels_mask=torch.stack(labels_mask)\n",
    "    tensor_ids_mask=torch.cat([torch.ones((len_data, 1)).long(),tensor_ids_mask,2*torch.ones((len_data, 1)).long()],dim=1)\n",
    "    labels_mask=torch.cat([-100*torch.ones((len_data, 1)).long(),labels_mask,-100*torch.ones((len_data, 1)).long()],dim=1)\n",
    "    attmask=torch.ones(tensor_ids_mask.size()).long()\n",
    "    encodings = {'input_ids': tensor_ids_mask, 'attention_mask': attmask, 'labels': labels_mask}\n",
    "    dataset = Dataset(encodings)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader=prepo_data(train,data_dir)\n",
    "val_loader=prepo_data(val,data_dir)\n",
    "test_loader=prepo_data(test,data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "config=BertConfig(vocab_size=1028,max_position_embeddings=258)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForMaskedLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(1028, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(258, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=1028, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "# and move our model over to the selected device\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# activate training mode\n",
    "model.train()\n",
    "# initialize optimizer\n",
    "optim = AdamW(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|| 216/216 [09:08<00:00,  2.54s/it, loss=6.13]\n",
      "Epoch 1: 100%|| 216/216 [08:52<00:00,  2.46s/it, loss=6.14]\n"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # setup loop with TQDM and dataloader\n",
    "    loop = tqdm(train_loader, total=len(train_loader))\n",
    "    for batch in loop:\n",
    "        # initialize calculated gradients (from prev step)\n",
    "        optim.zero_grad()\n",
    "        # pull all tensor batches required for training\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        # process\n",
    "        outputs = model(input_ids, attention_mask=attention_mask,\n",
    "                        labels=labels)\n",
    "        # extract loss\n",
    "        loss = outputs.loss\n",
    "        # calculate loss for every parameter that needs grad update\n",
    "        loss.backward()\n",
    "        # update parameters\n",
    "        optim.step()\n",
    "        # print relevant info to progress bar\n",
    "        loop.set_description(f'Epoch {epoch}')\n",
    "        loop.set_postfix(loss=loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p38C",
   "language": "python",
   "name": "p38c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
